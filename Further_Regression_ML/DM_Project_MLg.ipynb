{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2cb79b6d7c26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m from pandas.core.api import (\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;31m# dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mInt8Dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNamedAgg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_eng_float_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m from pandas.core.index import (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from pandas.core.groupby.generic import (  # noqa: F401\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mDataFrameGroupBy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mSeriesGroupBy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_shared_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   8491\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_series_or_dataframe_operations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8493\u001b[1;33m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_flex_arithmetic_methods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8494\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_special_arithmetic_methods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36madd_flex_arithmetic_methods\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[0mflex_arith_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflex_comp_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_method_wrappers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     new_methods = _create_methods(\n\u001b[1;32m--> 873\u001b[1;33m         \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflex_arith_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflex_comp_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    874\u001b[0m     )\n\u001b[0;32m    875\u001b[0m     new_methods.update(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36m_create_methods\u001b[1;34m(cls, arith_method, comp_method, bool_method, special)\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[0msub\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mmul\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m         \u001b[0mtruediv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m         \u001b[0mfloordiv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloordiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;31m# Causes a floating point exception in the tests when numexpr enabled,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36m_arith_method_FRAME\u001b[1;34m(cls, op, special)\u001b[0m\n\u001b[0;32m   1484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddendum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddendum\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddendum\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mdocitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddendum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdedent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\textwrap.py\u001b[0m in \u001b[0;36mdedent\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_whitespace_only_re\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[0mindents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_leading_whitespace_re\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmargin\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mmargin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import talib\n",
    "import statsmodels.api as sm\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tsa.arima_model import ARMA \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('matrix_second.csv', index_col='time')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "print(df.head())\n",
    "n_row, n_col = df.shape\n",
    "print(f'There are {n_row} rows and {n_col} columns')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already Identified previously, we have to deal with missing values (NaN) within our dataset. In our correlation analysis, we had backfilled our values. This time, let's follow a linear replacement method using the interpolate method(linear) that identifies all missing values and fills the values (a visualization will help make this clear) based on a linear path. We can also interpolate equivalent to the backfill method using # Interpolate using\n",
    "interpolation_type = 'zero'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_and_plot(df, interpolation):\n",
    "\n",
    "    # boolean mask for missing values\n",
    "    missing_values = df.isna()\n",
    "\n",
    "    # Interpolating missing values\n",
    "    prices_interp = df.interpolate(interpolation)\n",
    "\n",
    "    # plot while highlighting the interpolated values in black\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
    "    \n",
    "    # plot while interpolated values on top in red\n",
    "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear interpolation type\n",
    "interpolation_type = 'linear'\n",
    "interpolate_and_plot(df, interpolation_type)\n",
    "\n",
    "# boolean mask for missing values\n",
    "missing_values = df.isna()\n",
    "\n",
    "# Interpolating with missing values\n",
    "df = df.interpolate('linear')\n",
    "print(df.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In continuation of using Bitcoin and Litecoin from our correlation methods, lets visualize a scatterplot with color relating to time, encoding time as the color of each datapoint. Bitcoin and Litecoin have drastically varying coin values, so we expect to yield interesting results and scale the y-axis. When not comparing coins, the Bitcoin data will be used as it is the most popular and expensive currency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw time series plot of bitcoin and litecoin\n",
    "df['btc'].plot(label='BTC', legend=True)\n",
    "df['ltc'].plot(label='LTC', legend=True, secondary_y=True)\n",
    "plt.title('Raw time series plot')\n",
    "plt.show() \n",
    "plt.clf() \n",
    "\n",
    "# Histogram of the daily price change percent \n",
    "df['btc'].pct_change().plot.hist(bins=80)\n",
    "plt.axis([-.005,.007,0,25000])\n",
    "plt.xlabel('Percent change')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see a nearly normal distribution. Although we've done correlation analysis already, let's use Pearson's correlation coefficient to detect any linear relationships. We'll check the correlations between current price changes to see if previous price changes can predicture future ones. Below, based on the outputted correlation matrix between 5 day percentage changes (current and future), we can discern a slighly negative correlation (-0.0225) to the change in the last 5 days, an example of mean reversion (stock prices bounce around as opposed to following an upward trend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-min % price changes for the current day, and 5 min in the future\n",
    "df['5min_future'] = df['btc'].shift(-5)\n",
    "df['5min_future_pct'] = df['5min_future'].pct_change(5)\n",
    "df['5min_pct'] = df['btc'].pct_change(5)\n",
    "\n",
    "# correlation matrix between 5-min percentage changes (current and future)\n",
    "corr = df[['5min_pct', '5min_future_pct']].corr()\n",
    "print(corr)\n",
    "\n",
    "# Scatter plot for current 5-day percent change vs the future 5-day percent change\n",
    "plt.scatter(df['5min_pct'], df['5min_future_pct'])\n",
    "plt.show()\n",
    "\n",
    "# heatmap of correlation matrix\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.yticks(rotation=0); plt.xticks(rotation=90)  \n",
    "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
    "plt.show()  \n",
    "plt.clf() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix and scatter plot show almost no correlation but let's now shift 15, 30, an hour, and even 200 minutes into our dataset to see what interesting correlations exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['5min_pct'] \n",
    "\n",
    "# moving averages and rsi for timeperiods of 15, 30, 60, and 200 minutes \n",
    "# This analysis would improve when working with days or weeks versus minutes\n",
    "for n in [15, 30, 60, 200]:\n",
    "\n",
    "    # Create the moving average indicator \n",
    "    df['ma' + str(n)] = talib.SMA(df['bch'].values,\n",
    "                              timeperiod=n) / df['bch']\n",
    "    # Create the RSI indicator\n",
    "    df['rsi' + str(n)] = talib.RSI(df['bch'].values, timeperiod=n)\n",
    "    \n",
    "    # Add rsi and moving average to the feature name list\n",
    "    feature_names = feature_names + ['ma' + str(n), 'rsi' + str(n)]\n",
    "    \n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all nan values\n",
    "df = df.dropna()\n",
    "\n",
    "# features and targets\n",
    "# use feature_names for features; '5d_close_future_pct' for targets\n",
    "features = df[feature_names]\n",
    "targets = df['5min_future_pct']\n",
    "\n",
    "# DataFrame from target column and feature columns\n",
    "feature_and_target_cols = ['5min_future_pct'] + feature_names\n",
    "feat_targ_df = df[feature_and_target_cols]\n",
    "\n",
    "# correlation matrix\n",
    "corr = feat_targ_df.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some high correlations for relative strength index (RSI) rsi60 and rsi30, moving average (ma) ma60 and ma200, rsi15 and rsi30, and strong negative correlation between ma200 and rsi200, ma60 and rsi30. Compared to the target ['5min_future_pct'], rsi15 has the highest correlation at 0.066734."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of correlation matrix\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.yticks(rotation=0); plt.xticks(rotation=90)  # fix ticklabel directions\n",
    "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
    "plt.show()  # show the plot\n",
    "plt.clf() # clear the plot area\n",
    "\n",
    "# scatter plot of the most highly correlated variable with the target\n",
    "plt.scatter(df['rsi15'], df['5min_future_pct'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now set up training dataset to be used for preparing some prediction models and compare various performances between models discussed in our class such as decisions trees and k-nearest neighbors. Let's also take a look to see which features have the more importance in strengthening prediction models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a constant to the features\n",
    "linear_features = sm.add_constant(features)\n",
    "\n",
    "# size for the training set that is 85% of the number of samples\n",
    "train_size = int(0.85 * features.shape[0])\n",
    "train_features = linear_features[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "test_features = linear_features[train_size:]\n",
    "test_targets = targets[train_size:]\n",
    "print(linear_features.shape, train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model and least squares fit\n",
    "model = sm.OLS(train_targets, train_features)\n",
    "results = model.fit()  \n",
    "print(results.summary())\n",
    "\n",
    "# examine pvalues\n",
    "# Features with p <= 0.05 are typically considered significantly different from 0\n",
    "print(results.pvalues)\n",
    "\n",
    "# Make predictions from our model for train and test sets\n",
    "train_predictions = results.predict(train_features)\n",
    "test_predictions = results.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter the predictions vs the targets with 80% transparency\n",
    "plt.scatter(train_predictions, train_targets, alpha=0.2, color='b', label='train')\n",
    "plt.scatter(test_predictions, test_targets, alpha=0.2, color='r', label='test')\n",
    "\n",
    "# Plot the perfect prediction line\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(np.arange(xmin, xmax, 0.01), np.arange(xmin, xmax, 0.01), c='k')\n",
    "\n",
    "# Set the axis labels and show the plot\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('actual')\n",
    "plt.legend()# show the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different non-linear machine learning model and begin using decision trees. Decision trees will split our data into groups based on the features (set earlier). We will determine which max_depth can provide the best prediction without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# decision tree regression model with default arguments\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "# Checking score on train and test\n",
    "print(decision_tree.score(train_features, train_targets))\n",
    "print(decision_tree.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A near perfect fit on our training data, but our testing data isn't up to par. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through a few different max depths and check the performance\n",
    "for d in [3,5,10]:\n",
    "    # fitting decision tree \n",
    "    decision_tree = DecisionTreeRegressor(max_depth = d)\n",
    "    decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "    # scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree.score(train_features, train_targets))\n",
    "    print(decision_tree.score(test_features, test_targets), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the best fit is a max depth of 3 at a score of -0.02125. Below, we would want to see diagonal lines from the lower left to the upper right. However, due to the simplistic nature of decisions trees, our model is not going to do well on the test set. But it will do well on the train set. As seen below, the predictions group into lines due our limited depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using best max_depth of 3 to fit a decision tree\n",
    "decision_tree = DecisionTreeRegressor(max_depth=3)\n",
    "decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "# Prediction values for train and test\n",
    "train_predictions = decision_tree.predict(train_features)\n",
    "test_predictions = decision_tree.predict(test_features)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_predictions, train_targets, label='train')\n",
    "plt.scatter(test_predictions, test_targets, label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've examined decision trees, let's briefly look at a random forest model. We will use random sample of training data points to test our results. Let's now examine a random forest model and determine its use for prediction on our dataset. Here, a random forest is made up of multiple decision trees (Reference: https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the random forest model and fit to the training data\n",
    "rfr = RandomForestRegressor(n_estimators=200)\n",
    "rfr.fit(train_features, train_targets)\n",
    "\n",
    "# Look at the R^2 scores on train and test\n",
    "print(rfr.score(train_features, train_targets))\n",
    "print(rfr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use sklearn's GridSearchCV() method to search hyperparameters, but with a financial time series, we don't want to do cross-validation due to data mixing. We want to fit our models on the oldest data and evaluate on the newest data. So we'll use sklearn's ParameterGrid to create combinations of hyperparameters to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# dictionary of hyperparameters to search\n",
    "grid = {'n_estimators': [200], 'max_depth': [3], 'max_features': [4, 8], 'random_state': [42]}\n",
    "test_scores = []\n",
    "\n",
    "# Loop through parameter grid, set the hyperparameters, and save the scores\n",
    "for g in ParameterGrid(grid):\n",
    "    rfr.set_params(**g)  # ** is used to unpack the dictionary\n",
    "    rfr.fit(train_features, train_targets)\n",
    "    test_scores.append(rfr.score(test_features, test_targets))\n",
    "\n",
    "# best hyperparameters from the test score\n",
    "best_idx = np.argmax(test_scores)\n",
    "print(test_scores[best_idx], ParameterGrid(grid)[best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best hyperparameters from before to fit a random forest model\n",
    "rfr = RandomForestRegressor(n_estimators=200, max_depth=3, max_features=4, random_state=42)\n",
    "rfr.fit(train_features, train_targets)\n",
    "\n",
    "# predictions with our model\n",
    "train_predictions = rfr.predict(train_features)\n",
    "test_predictions = rfr.predict(test_features)\n",
    "\n",
    "# scatter plot with train/test actual vs predictions\n",
    "plt.scatter(train_targets, train_predictions, label='train')\n",
    "plt.scatter(test_targets, test_predictions, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from our random forest model\n",
    "importances = rfr.feature_importances_\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x = range(len(importances))\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index-1]\n",
    "plt.bar(x, importances[sorted_index], tick_label=labels)\n",
    "\n",
    "# Rotate tick labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see moving average 200 and 15 minutes along with relative strength index 200 minutes as the most contributing features to the predictions. Let's focus on these features to possibly enhance/tune them for even strong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create GB model -- hyperparameters have already been searched for you\n",
    "gbr = GradientBoostingRegressor(max_features=4,\n",
    "                                learning_rate=0.01,\n",
    "                                n_estimators=200,\n",
    "                                subsample=0.6,\n",
    "                                random_state=42)\n",
    "gbr.fit(train_features, train_targets)\n",
    "\n",
    "print(gbr.score(train_features, train_targets))\n",
    "print(gbr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From random forest, the scores for train/test were 0.8733246218778411 and -0.08374728278454358. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from the fitted gradient boosting model\n",
    "feature_importances = gbr.feature_importances_\n",
    "\n",
    "# Get the indices of the largest to smallest feature importances\n",
    "sorted_index = np.argsort(feature_importances)[::-1]\n",
    "x = range(len(feature_importances))\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index-1]\n",
    "\n",
    "plt.bar(x, feature_importances[sorted_index], tick_label=labels)\n",
    "\n",
    "# tick lables corresponding to feature names \n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try k nearest neighbors method. As discussed in lecture, KNN takes the k-nearest points to a new point and averages their target values to obtain a prediction. By implementing scale() method, we will try normalizing the data so the features have similar ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Standardize the train and test features\n",
    "scaled_train_features = scale(train_features)\n",
    "scaled_test_features = scale(test_features)\n",
    "\n",
    "# histogram before and after scaling (model might work better with dataset spread over different timeframe)\n",
    "f, ax = plt.subplots(nrows=2, ncols=1)\n",
    "train_features.iloc[:, 2].hist(ax=ax[0])\n",
    "ax[1].hist(scaled_train_features[:, 2])\n",
    "plt.show()\n",
    "\n",
    "for n in range(30, 38):\n",
    "    # fitting the KNN model\n",
    "    knn = KNeighborsRegressor(n_neighbors=n)\n",
    "    \n",
    "    # Fitting model to the training data\n",
    "    knn.fit(scaled_train_features, train_targets)\n",
    "    \n",
    "    # number of neighbors score of best value of n \n",
    "    print(\"n_neighbors =\", n)\n",
    "    print('train, test scores')\n",
    "    print(knn.score(scaled_train_features, train_targets))\n",
    "    print(knn.score(scaled_test_features, test_targets))\n",
    "    print()  # prints a blank line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plugging and testing a various range of values, we can choose n-neighbors 34 for optimal model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best-performing n_neighbors of 34\n",
    "knn = KNeighborsRegressor(n_neighbors=34)\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(scaled_train_features, train_targets)\n",
    "\n",
    "# predictions for train and test sets\n",
    "train_predictions = knn.predict(scaled_train_features)\n",
    "test_predictions = knn.predict(scaled_test_features)\n",
    "\n",
    "# plotting actual vs predicted values\n",
    "plt.scatter(train_predictions, train_targets, label='train')\n",
    "plt.scatter(test_predictions, test_targets, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we wanted to toy around with keras and neural nets.\n",
    "--ran out of time --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create the model\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_1.add(Dense(20, activation='relu'))\n",
    "model_1.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Fit the model\n",
    "model_1.compile(optimizer='adam', loss='mse')\n",
    "history = model_1.fit(scaled_train_features, train_targets, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('btc', 'ltc', c=df.index, \n",
    "                    cmap=plt.cm.viridis, colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Use stock symbols to extract training data\n",
    "X = df[['bch', 'eth', 'ltc', 'xpr']]\n",
    "y = df[['btc']]\n",
    "\n",
    "# Fit and score the model with cross-validation\n",
    "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}